{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Research Agent\n",
        "\n",
        "# Overview\n",
        "\n",
        "The Medical Research Agent is an AI-powered system designed to process complex medical text and provide clear and structured insights.\n",
        "\n",
        "It can help with understanding:\n",
        "\n",
        "Medical Conditions- diseases, disorders and related issues\n",
        "\n",
        "Medicines- drugs, treatments and their uses\n",
        "\n",
        "Symptoms- what they may indicate and when to be cautious\n",
        "\n",
        "Treatments- available options and important considerations\n",
        "\n",
        "# How It Works\n",
        "\n",
        "Text Understanding- The agent simplifies complex medical text into clear summaries.\n",
        "\n",
        "Information Extraction- It identifies symptoms, causes, and treatment details from the input.\n",
        "\n",
        "Web Retrieval (RAG)- The system fetches verified medical definitions using AI-powered web search.\n",
        "\n",
        "Structured Report Generation- All information is combined into a clean and easy-to-read medical report.\n",
        "\n",
        "# Medical Disclaimer\n",
        "\n",
        "This tool is for educational and informational purposes only.\n",
        "It should not be used as a substitute for professional medical advice, diagnosis, or treatment.\n",
        "Always consult a qualified healthcare provider for medical concerns."
      ],
      "metadata": {
        "id": "CRas6pFem3Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "In this step, we will install all the required packages:"
      ],
      "metadata": {
        "id": "rLKWF6mynSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python langchain-tavily wikipedia"
      ],
      "metadata": {
        "id": "odSDT6lhsHl4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the API Keys\n",
        "\n",
        "So for this project, we will need these 2 API keys:\n",
        "\n",
        "1. **OpenAI API Key**: I got this from https://platform.openai.com/api-keys\n",
        "2. **Tavily API Key**: I got this from https://tavily.com"
      ],
      "metadata": {
        "id": "dVlZ8E78niAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijYqmWuMnUXp",
        "outputId": "b5883ea5-3ec7-40d0-fa95-e44c30849008"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "TAVILY_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies\n"
      ],
      "metadata": {
        "id": "IxKZ-ELHnp7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, get_buffer_string\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from IPython.display import display, HTML, Image\n",
        "import operator\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Initialize Tavily Search\n",
        "tavily_search = TavilySearch(max_results=3)\n",
        "\n",
        "print(\"All dependencies are loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xzfqweip5DI",
        "outputId": "d1f8d274-8f2b-4846-e512-c99701d5a82c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Medical Analyst Models\n",
        "\n",
        "We will create specialized medical analysts:"
      ],
      "metadata": {
        "id": "KfpKHXmewfxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalAnalyst(BaseModel):\n",
        "    \"\"\"Medical specialist analyst\"\"\"\n",
        "    affiliation: str = Field(description=\"Medical affiliation or specialty\")\n",
        "    name: str = Field(description=\"Name of the medical analyst\")\n",
        "    role: str = Field(description=\"Medical role or specialty area\")\n",
        "    description: str = Field(description=\"Focus area, concerns, and medical expertise\")\n",
        "\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "class MedicalPerspectives(BaseModel):\n",
        "    analysts: List[MedicalAnalyst] = Field(\n",
        "        description=\"List of medical analysts with their specialties\"\n",
        "    )\n",
        "\n",
        "class GenerateAnalystsState(TypedDict):\n",
        "    topic: str  # Medical topic or condition\n",
        "    max_analysts: int  # Number of analysts\n",
        "    human_analyst_feedback: str  # Human feedback\n",
        "    analysts: List[MedicalAnalyst]  # Generated analysts\n",
        "\n",
        "print(\"Medical analyst models defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvAjZep5KtEO",
        "outputId": "12e7860f-e71a-4ca6-a865-1f72d20777d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst models defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Medical Analysts\n",
        "\n",
        "Generating specialized medical analysts for different aspects of the condition:"
      ],
      "metadata": {
        "id": "PkiV8YQ6K02U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_instructions = \"\"\"You are tasked with creating a set of medical specialist personas to research a health topic.\n",
        "\n",
        "1. Review the medical topic: {topic}\n",
        "\n",
        "2. Consider any feedback: {human_analyst_feedback}\n",
        "\n",
        "3. Determine the most important medical perspectives (symptoms, treatments, prevention, prognosis, causes, etc.)\n",
        "\n",
        "4. Create {max_analysts} medical specialists, each focusing on a different aspect.\n",
        "\n",
        "Example specialists:\n",
        "- Symptomatologist (focuses on symptoms and diagnosis)\n",
        "- Treatment Specialist (focuses on treatment options)\n",
        "- Prevention Expert (focuses on prevention and risk factors)\n",
        "- Pharmacologist (focuses on medications)\n",
        "\"\"\"\n",
        "\n",
        "def create_medical_analysts(state: GenerateAnalystsState):\n",
        "    \"\"\"Create medical analyst personas\"\"\"\n",
        "    topic = state['topic']\n",
        "    max_analysts = state['max_analysts']\n",
        "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
        "\n",
        "    structured_llm = llm.with_structured_output(MedicalPerspectives)\n",
        "    system_message = analyst_instructions.format(\n",
        "        topic=topic,\n",
        "        human_analyst_feedback=human_analyst_feedback,\n",
        "        max_analysts=max_analysts\n",
        "    )\n",
        "\n",
        "    analysts = structured_llm.invoke([\n",
        "        SystemMessage(content=system_message),\n",
        "        HumanMessage(content=\"Generate the medical specialist analysts.\")\n",
        "    ])\n",
        "\n",
        "    return {\"analysts\": analysts.analysts}\n",
        "\n",
        "def should_continue(state: GenerateAnalystsState):\n",
        "    \"\"\"Check if we should continue or end\"\"\"\n",
        "    if state.get('human_analyst_feedback', None):\n",
        "        return \"create_analysts\"\n",
        "    return END\n",
        "\n",
        "# Build analyst generation graph\n",
        "builder = StateGraph(GenerateAnalystsState)\n",
        "builder.add_node(\"create_analysts\", create_medical_analysts)\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_conditional_edges(\"create_analysts\", should_continue, [\"create_analysts\", END])\n",
        "\n",
        "memory = MemorySaver()\n",
        "analyst_graph = builder.compile(checkpointer=memory)\n",
        "\n",
        "print(\"Medical analyst generation system is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNFqYcwVLNI1",
        "outputId": "d9d829cd-ad39-4631-e611-22418a8bffcf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst generation system is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Interview System\n",
        "\n",
        "Setting up the interview system where the analysts can collect information from medical experts:"
      ],
      "metadata": {
        "id": "9OBwGQYVMd4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class InterviewState(MessagesState):\n",
        "    max_num_turns: int  # Number of interview turns\n",
        "    context: Annotated[list, operator.add]  # Web search results\n",
        "    analyst: MedicalAnalyst  # The analyst\n",
        "    interview: str  # Interview transcript\n",
        "    sections: list  # Final sections\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(description=\"Medical search query for web research\")\n",
        "\n",
        "print(\"Interview state models are defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sp3q8eMMtKh",
        "outputId": "6378de1b-9107-4ecd-c846-933b1a6e01d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interview state models are defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of Questions\n",
        "\n",
        "The Analysts will generate many relevant questions to ask medical experts:"
      ],
      "metadata": {
        "id": "WYwsD3QRGV2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_instructions = \"\"\"You are a medical analyst interviewing an expert about a health topic.\n",
        "\n",
        "Your goal is to gather specific, evidence-based medical insights.\n",
        "\n",
        "1. Focus on: {goals}\n",
        "\n",
        "2. Ask specific questions about:\n",
        "   - Clinical evidence and research\n",
        "   - Treatment efficacy and safety\n",
        "   - Patient outcomes\n",
        "   - Current medical guidelines\n",
        "\n",
        "3. Avoid vague questions - be specific and clinical\n",
        "\n",
        "Begin by introducing yourself, then ask your medical question.\n",
        "\n",
        "When satisfied, end with: \"Thank you so much for your help!\"\n",
        "\"\"\"\n",
        "\n",
        "def generate_question(state: InterviewState):\n",
        "    \"\"\"Generate analyst question\"\"\"\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    system_message = question_instructions.format(goals=analyst.persona)\n",
        "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    return {\"messages\": [question]}\n",
        "\n",
        "print(\"The Question generation process is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg8SjaEa3HRb",
        "outputId": "b3020588-e7f4-4490-f47c-3bb7225d877e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Question generation process is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Search Functions\n",
        "\n",
        "Now, we will search the web and Wikipedia for getting medical information:"
      ],
      "metadata": {
        "id": "rZGl72hyHOuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "search_instructions = SystemMessage(content=\"\"\"Given a medical conversation, generate a search query to find evidence-based medical information.\n",
        "\n",
        "Focus on:\n",
        "- Peer-reviewed medical sources\n",
        "- Clinical guidelines\n",
        "- Medical research\n",
        "- Reputable health organizations\n",
        "\n",
        "Create a precise medical search query based on the conversation.\"\"\")\n",
        "\n",
        "def search_web(state: InterviewState):\n",
        "    \"\"\"Search web for medical information\"\"\"\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
        "\n",
        "    # Perform web search\n",
        "    try:\n",
        "        search_results = tavily_search.invoke(search_query.search_query)\n",
        "\n",
        "        # Handle different response formats\n",
        "        if isinstance(search_results, list):\n",
        "            search_docs = search_results\n",
        "        elif isinstance(search_results, dict):\n",
        "            search_docs = search_results.get(\"results\", [])\n",
        "        else:\n",
        "            search_docs = []\n",
        "\n",
        "        # Format results\n",
        "        if search_docs:\n",
        "            formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
        "                f'<Document href=\"{doc.get(\"url\", \"N/A\")}\"/>\\n{doc.get(\"content\", doc.get(\"snippet\", \"\"))}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "                if isinstance(doc, dict)\n",
        "            ])\n",
        "        else:\n",
        "            formatted_search_docs = \"No search results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Search error: {e}\")\n",
        "        formatted_search_docs = f\"Search error occurred: {str(e)}\"\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "def search_wikipedia(state: InterviewState):\n",
        "    \"\"\"Search Wikipedia for medical information\"\"\"\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
        "\n",
        "    try:\n",
        "        # Search Wikipedia\n",
        "        search_docs = WikipediaLoader(query=search_query.search_query, load_max_docs=2).load()\n",
        "\n",
        "        # Format results\n",
        "        if search_docs:\n",
        "            formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
        "                f'<Document source=\"{doc.metadata.get(\"source\", \"Wikipedia\")}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "            ])\n",
        "        else:\n",
        "            formatted_search_docs = \"No Wikipedia results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Wikipedia search error: {e}\")\n",
        "        formatted_search_docs = f\"Wikipedia search error: {str(e)}\"\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "print(\" Fixed search functions loaded!\")\n",
        "print(\" Now I will need to rebuild the interview graph - run the 'Build Interview Graph' cell again!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnoB4OmNHMBL",
        "outputId": "f6a9fddb-111c-4008-ba66-786b0c306508"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fixed search functions loaded!\n",
            " Now I will need to rebuild the interview graph - run the 'Build Interview Graph' cell again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Medical Expert Answers\n",
        "\n",
        "The medical expert uses web-searched information to answer questions:"
      ],
      "metadata": {
        "id": "HseaaviS3wTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_instructions = \"\"\"You are a medical expert being interviewed.\n",
        "\n",
        "Analyst focus: {goals}\n",
        "\n",
        "Use ONLY the provided medical sources to answer: {context}\n",
        "\n",
        "Guidelines:\n",
        "1. Base answers on evidence from the provided sources\n",
        "2. Cite sources using [1], [2], etc.\n",
        "3. Be specific and clinical\n",
        "4. Note limitations if sources are insufficient\n",
        "5. List sources at the end\n",
        "\n",
        "Provide accurate, evidence-based medical information.\"\"\"\n",
        "\n",
        "def generate_answer(state: InterviewState):\n",
        "    \"\"\"Generate expert answer\"\"\"\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
        "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "    answer.name = \"medical_expert\"\n",
        "\n",
        "    return {\"messages\": [answer]}\n",
        "\n",
        "def save_interview(state: InterviewState):\n",
        "    \"\"\"Save interview transcript\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    interview = get_buffer_string(messages)\n",
        "    return {\"interview\": interview}\n",
        "\n",
        "def route_messages(state: InterviewState, name: str = \"medical_expert\"):\n",
        "    \"\"\"Route between questions and answers\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_num_turns = state.get('max_num_turns', 2)\n",
        "\n",
        "    # Count expert responses\n",
        "    num_responses = len([m for m in messages if isinstance(m, AIMessage) and m.name == name])\n",
        "\n",
        "    if num_responses >= max_num_turns:\n",
        "        return 'save_interview'\n",
        "\n",
        "    # Check for end signal\n",
        "    if len(messages) >= 2:\n",
        "        last_question = messages[-2]\n",
        "        if \"Thank you so much for your help\" in last_question.content:\n",
        "            return 'save_interview'\n",
        "\n",
        "    return \"ask_question\"\n",
        "\n",
        "print(\"Answer generation system ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yAcQV0M35Wd",
        "outputId": "20731eab-896f-4540-9291-715212df85c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer generation system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Interview Graph"
      ],
      "metadata": {
        "id": "PWZ4SOvd4BEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Building the interview graph\n",
        "interview_builder = StateGraph(InterviewState)\n",
        "interview_builder.add_node(\"ask_question\", generate_question)\n",
        "interview_builder.add_node(\"search_web\", search_web)\n",
        "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
        "interview_builder.add_node(\"answer_question\", generate_answer)\n",
        "interview_builder.add_node(\"save_interview\", save_interview)\n",
        "\n",
        "# Defining the flow\n",
        "interview_builder.add_edge(START, \"ask_question\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
        "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
        "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
        "interview_builder.add_conditional_edges(\"answer_question\", route_messages, [\"ask_question\", \"save_interview\"])\n",
        "interview_builder.add_edge(\"save_interview\", END)\n",
        "\n",
        "interview_graph = interview_builder.compile()\n",
        "\n",
        "print(\"Interview graph is compiled!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuoQZ_Jc4FFD",
        "outputId": "2ddce5e5-6b6f-4b6c-c7b3-82bd1500aeee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interview graph is compiled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report Generation\n",
        "\n",
        "Generating the final medical report from all the interviews:"
      ],
      "metadata": {
        "id": "ROSNWwYx5N4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_writer_instructions = \"\"\"You are a medical writer creating a section of a medical report.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Your focus: {focus}\n",
        "\n",
        "Interview transcript: {interview}\n",
        "\n",
        "Create a well-structured section with:\n",
        "1. Clear medical terminology\n",
        "2. Evidence-based information\n",
        "3. Proper citations [1], [2], etc.\n",
        "4. Clinical relevance\n",
        "\n",
        "Write in a professional medical style.\"\"\"\n",
        "\n",
        "def write_section(interview: str, analyst: MedicalAnalyst, topic: str):\n",
        "    \"\"\"Write a section based on interview\"\"\"\n",
        "    system_message = section_writer_instructions.format(\n",
        "        topic=topic,\n",
        "        focus=analyst.description,\n",
        "        interview=interview\n",
        "    )\n",
        "    section = llm.invoke([SystemMessage(content=system_message)])\n",
        "    return section.content\n",
        "\n",
        "report_writer_instructions = \"\"\"You are a medical report compiler creating a comprehensive medical document.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Sections from specialists:\n",
        "{sections}\n",
        "\n",
        "Create a comprehensive medical report with:\n",
        "\n",
        "# {topic}: Complete Medical Overview\n",
        "\n",
        "## Executive Summary\n",
        "[Brief overview]\n",
        "\n",
        "## Detailed Analysis\n",
        "[Integrate all sections logically]\n",
        "\n",
        "## Key Takeaways\n",
        "[Important points]\n",
        "\n",
        "## Sources\n",
        "[All citations]\n",
        "\n",
        "**MEDICAL DISCLAIMER**: This information is for educational purposes only...\n",
        "\"\"\"\n",
        "\n",
        "def compile_report(topic: str, sections: List[str]):\n",
        "    \"\"\"Compile final medical report\"\"\"\n",
        "    formatted_sections = \"\\n\\n\".join(sections)\n",
        "    system_message = report_writer_instructions.format(\n",
        "        topic=topic,\n",
        "        sections=formatted_sections\n",
        "    )\n",
        "    report = llm.invoke([SystemMessage(content=system_message)])\n",
        "    return report.content\n",
        "\n",
        "print(\"The Report generation system is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQED5RXJ5Uiy",
        "outputId": "5b4b3de0-d1ca-4580-b579-1b1f570d3ae2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Report generation system is ready!\n"
          ]
        }
      ]
    }
  ]
}