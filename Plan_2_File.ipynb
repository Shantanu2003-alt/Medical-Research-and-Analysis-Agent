{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Research Agent\n",
        "\n",
        "# Overview\n",
        "\n",
        "The Medical Research Agent is an AI-powered system designed to process complex medical text and provide clear and structured insights.\n",
        "\n",
        "It can help with understanding:\n",
        "\n",
        "Medical Conditions- diseases, disorders and related issues\n",
        "\n",
        "Medicines- drugs, treatments and their uses\n",
        "\n",
        "Symptoms- what they may indicate and when to be cautious\n",
        "\n",
        "Treatments- available options and important considerations\n",
        "\n",
        "# How It Works\n",
        "\n",
        "Text Understanding- The agent simplifies complex medical text into clear summaries.\n",
        "\n",
        "Information Extraction- It identifies symptoms, causes, and treatment details from the input.\n",
        "\n",
        "Web Retrieval (RAG)- The system fetches verified medical definitions using AI-powered web search.\n",
        "\n",
        "Structured Report Generation- All information is combined into a clean and easy-to-read medical report.\n",
        "\n",
        "# Medical Disclaimer\n",
        "\n",
        "This tool is for educational and informational purposes only.\n",
        "It should not be used as a substitute for professional medical advice, diagnosis, or treatment.\n",
        "Always consult a qualified healthcare provider for medical concerns."
      ],
      "metadata": {
        "id": "CRas6pFem3Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "In this step, we will install all the required packages:"
      ],
      "metadata": {
        "id": "rLKWF6mynSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python langchain-tavily wikipedia"
      ],
      "metadata": {
        "id": "odSDT6lhsHl4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the API Keys\n",
        "\n",
        "So for this project, we will need these 2 API keys:\n",
        "\n",
        "1. **OpenAI API Key**: I got this from https://platform.openai.com/api-keys\n",
        "2. **Tavily API Key**: I got this from https://tavily.com"
      ],
      "metadata": {
        "id": "dVlZ8E78niAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijYqmWuMnUXp",
        "outputId": "824eb56d-47a2-4d0e-b94f-7fa2df4c2c57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "TAVILY_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies\n"
      ],
      "metadata": {
        "id": "IxKZ-ELHnp7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, get_buffer_string\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from IPython.display import display, HTML, Image\n",
        "import operator\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Initialize Tavily Search\n",
        "tavily_search = TavilySearch(max_results=3)\n",
        "\n",
        "print(\"All dependencies are loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xzfqweip5DI",
        "outputId": "16963f74-801f-4fe6-a287-a1f67db0cd0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Medical Analyst Models\n",
        "\n",
        "We will create specialized medical analysts:"
      ],
      "metadata": {
        "id": "KfpKHXmewfxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalAnalyst(BaseModel):\n",
        "    \"\"\"Medical specialist analyst\"\"\"\n",
        "    affiliation: str = Field(description=\"Medical affiliation or specialty\")\n",
        "    name: str = Field(description=\"Name of the medical analyst\")\n",
        "    role: str = Field(description=\"Medical role or specialty area\")\n",
        "    description: str = Field(description=\"Focus area, concerns, and medical expertise\")\n",
        "\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "class MedicalPerspectives(BaseModel):\n",
        "    analysts: List[MedicalAnalyst] = Field(\n",
        "        description=\"List of medical analysts with their specialties\"\n",
        "    )\n",
        "\n",
        "class GenerateAnalystsState(TypedDict):\n",
        "    topic: str  # Medical topic or condition\n",
        "    max_analysts: int  # Number of analysts\n",
        "    human_analyst_feedback: str  # Human feedback\n",
        "    analysts: List[MedicalAnalyst]  # Generated analysts\n",
        "\n",
        "print(\"Medical analyst models defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvAjZep5KtEO",
        "outputId": "460871a0-da39-487c-f409-b92e4a52df3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst models defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Medical Analysts\n",
        "\n",
        "Generating specialized medical analysts for different aspects of the condition:"
      ],
      "metadata": {
        "id": "PkiV8YQ6K02U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_instructions = \"\"\"You are tasked with creating a set of medical specialist personas to research a health topic.\n",
        "\n",
        "1. Review the medical topic: {topic}\n",
        "\n",
        "2. Consider any feedback: {human_analyst_feedback}\n",
        "\n",
        "3. Determine the most important medical perspectives (symptoms, treatments, prevention, prognosis, causes, etc.)\n",
        "\n",
        "4. Create {max_analysts} medical specialists, each focusing on a different aspect.\n",
        "\n",
        "Example specialists:\n",
        "- Symptomatologist (focuses on symptoms and diagnosis)\n",
        "- Treatment Specialist (focuses on treatment options)\n",
        "- Prevention Expert (focuses on prevention and risk factors)\n",
        "- Pharmacologist (focuses on medications)\n",
        "\"\"\"\n",
        "\n",
        "def create_medical_analysts(state: GenerateAnalystsState):\n",
        "    \"\"\"Create medical analyst personas\"\"\"\n",
        "    topic = state['topic']\n",
        "    max_analysts = state['max_analysts']\n",
        "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
        "\n",
        "    structured_llm = llm.with_structured_output(MedicalPerspectives)\n",
        "    system_message = analyst_instructions.format(\n",
        "        topic=topic,\n",
        "        human_analyst_feedback=human_analyst_feedback,\n",
        "        max_analysts=max_analysts\n",
        "    )\n",
        "\n",
        "    analysts = structured_llm.invoke([\n",
        "        SystemMessage(content=system_message),\n",
        "        HumanMessage(content=\"Generate the medical specialist analysts.\")\n",
        "    ])\n",
        "\n",
        "    return {\"analysts\": analysts.analysts}\n",
        "\n",
        "def should_continue(state: GenerateAnalystsState):\n",
        "    \"\"\"Check if we should continue or end\"\"\"\n",
        "    if state.get('human_analyst_feedback', None):\n",
        "        return \"create_analysts\"\n",
        "    return END\n",
        "\n",
        "# Build analyst generation graph\n",
        "builder = StateGraph(GenerateAnalystsState)\n",
        "builder.add_node(\"create_analysts\", create_medical_analysts)\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_conditional_edges(\"create_analysts\", should_continue, [\"create_analysts\", END])\n",
        "\n",
        "memory = MemorySaver()\n",
        "analyst_graph = builder.compile(checkpointer=memory)\n",
        "\n",
        "print(\"Medical analyst generation system is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNFqYcwVLNI1",
        "outputId": "da693dd4-7410-4e58-8b9c-c9276cc1fcf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst generation system is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Interview System\n",
        "\n",
        "Setting up the interview system where the analysts can collect information from medical experts:"
      ],
      "metadata": {
        "id": "9OBwGQYVMd4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class InterviewState(MessagesState):\n",
        "    max_num_turns: int  # Number of interview turns\n",
        "    context: Annotated[list, operator.add]  # Web search results\n",
        "    analyst: MedicalAnalyst  # The analyst\n",
        "    interview: str  # Interview transcript\n",
        "    sections: list  # Final sections\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(description=\"Medical search query for web research\")\n",
        "\n",
        "print(\"Interview state models are defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sp3q8eMMtKh",
        "outputId": "78a51203-7b3a-4192-96d7-0e2246eb99e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interview state models are defined!\n"
          ]
        }
      ]
    }
  ]
}