{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Research Agent\n",
        "\n",
        "# Overview\n",
        "\n",
        "The Medical Research Agent is an AI-powered system designed to process complex medical text and provide clear and structured insights.\n",
        "\n",
        "It can help with understanding:\n",
        "\n",
        "Medical Conditions- diseases, disorders and related issues\n",
        "\n",
        "Medicines- drugs, treatments and their uses\n",
        "\n",
        "Symptoms- what they may indicate and when to be cautious\n",
        "\n",
        "Treatments- available options and important considerations\n",
        "\n",
        "# How It Works\n",
        "\n",
        "Text Understanding- The agent simplifies complex medical text into clear summaries.\n",
        "\n",
        "Information Extraction- It identifies symptoms, causes, and treatment details from the input.\n",
        "\n",
        "Web Retrieval (RAG)- The system fetches verified medical definitions using AI-powered web search.\n",
        "\n",
        "Structured Report Generation- All information is combined into a clean and easy-to-read medical report.\n",
        "\n",
        "# Medical Disclaimer\n",
        "\n",
        "This tool is for educational and informational purposes only.\n",
        "It should not be used as a substitute for professional medical advice, diagnosis, or treatment.\n",
        "Always consult a qualified healthcare provider for medical concerns."
      ],
      "metadata": {
        "id": "CRas6pFem3Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "In this step, we will install all the required packages:"
      ],
      "metadata": {
        "id": "rLKWF6mynSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python langchain-tavily wikipedia"
      ],
      "metadata": {
        "id": "odSDT6lhsHl4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the API Keys\n",
        "\n",
        "So for this project, we will need these 2 API keys:\n",
        "\n",
        "1. **OpenAI API Key**: I got this from https://platform.openai.com/api-keys\n",
        "2. **Tavily API Key**: I got this from https://tavily.com"
      ],
      "metadata": {
        "id": "dVlZ8E78niAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "ijYqmWuMnUXp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies\n"
      ],
      "metadata": {
        "id": "IxKZ-ELHnp7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, get_buffer_string\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from IPython.display import display, HTML, Image\n",
        "import operator\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Initialize Tavily Search\n",
        "tavily_search = TavilySearch(max_results=3)\n",
        "\n",
        "print(\"All dependencies are loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xzfqweip5DI",
        "outputId": "8a863b13-83e4-49a0-d1d1-d398320b0638"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Medical Analyst Models\n",
        "\n",
        "We will create specialized medical analysts:"
      ],
      "metadata": {
        "id": "KfpKHXmewfxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalAnalyst(BaseModel):\n",
        "    \"\"\"Medical specialist analyst\"\"\"\n",
        "    affiliation: str = Field(description=\"Medical affiliation or specialty\")\n",
        "    name: str = Field(description=\"Name of the medical analyst\")\n",
        "    role: str = Field(description=\"Medical role or specialty area\")\n",
        "    description: str = Field(description=\"Focus area, concerns, and medical expertise\")\n",
        "\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "class MedicalPerspectives(BaseModel):\n",
        "    analysts: List[MedicalAnalyst] = Field(\n",
        "        description=\"List of medical analysts with their specialties\"\n",
        "    )\n",
        "\n",
        "class GenerateAnalystsState(TypedDict):\n",
        "    topic: str  # Medical topic or condition\n",
        "    max_analysts: int  # Number of analysts\n",
        "    human_analyst_feedback: str  # Human feedback\n",
        "    analysts: List[MedicalAnalyst]  # Generated analysts\n",
        "\n",
        "print(\"Medical analyst models defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvAjZep5KtEO",
        "outputId": "922a1812-2fed-4d9a-b423-ecd96a1a9a13"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst models defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Medical Analysts\n",
        "\n",
        "Generating specialized medical analysts for different aspects of the condition:"
      ],
      "metadata": {
        "id": "PkiV8YQ6K02U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_instructions = \"\"\"You are tasked with creating a set of medical specialist personas to research a health topic.\n",
        "\n",
        "1. Review the medical topic: {topic}\n",
        "\n",
        "2. Consider any feedback: {human_analyst_feedback}\n",
        "\n",
        "3. Determine the most important medical perspectives (symptoms, treatments, prevention, prognosis, causes, etc.)\n",
        "\n",
        "4. Create {max_analysts} medical specialists, each focusing on a different aspect.\n",
        "\n",
        "Example specialists:\n",
        "- Symptomatologist (focuses on symptoms and diagnosis)\n",
        "- Treatment Specialist (focuses on treatment options)\n",
        "- Prevention Expert (focuses on prevention and risk factors)\n",
        "- Pharmacologist (focuses on medications)\n",
        "\"\"\"\n",
        "\n",
        "def create_medical_analysts(state: GenerateAnalystsState):\n",
        "    \"\"\"Create medical analyst personas\"\"\"\n",
        "    topic = state['topic']\n",
        "    max_analysts = state['max_analysts']\n",
        "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
        "\n",
        "    structured_llm = llm.with_structured_output(MedicalPerspectives)\n",
        "    system_message = analyst_instructions.format(\n",
        "        topic=topic,\n",
        "        human_analyst_feedback=human_analyst_feedback,\n",
        "        max_analysts=max_analysts\n",
        "    )\n",
        "\n",
        "    analysts = structured_llm.invoke([\n",
        "        SystemMessage(content=system_message),\n",
        "        HumanMessage(content=\"Generate the medical specialist analysts.\")\n",
        "    ])\n",
        "\n",
        "    return {\"analysts\": analysts.analysts}\n",
        "\n",
        "def should_continue(state: GenerateAnalystsState):\n",
        "    \"\"\"Check if we should continue or end\"\"\"\n",
        "    if state.get('human_analyst_feedback', None):\n",
        "        return \"create_analysts\"\n",
        "    return END\n",
        "\n",
        "# Build analyst generation graph\n",
        "builder = StateGraph(GenerateAnalystsState)\n",
        "builder.add_node(\"create_analysts\", create_medical_analysts)\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_conditional_edges(\"create_analysts\", should_continue, [\"create_analysts\", END])\n",
        "\n",
        "memory = MemorySaver()\n",
        "analyst_graph = builder.compile(checkpointer=memory)\n",
        "\n",
        "print(\"Medical analyst generation system is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNFqYcwVLNI1",
        "outputId": "272ded5f-4a9a-47c5-a015-73fd7cce19c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst generation system is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Interview System\n",
        "\n",
        "Setting up the interview system where the analysts can collect information from medical experts:"
      ],
      "metadata": {
        "id": "9OBwGQYVMd4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class InterviewState(MessagesState):\n",
        "    max_num_turns: int  # Number of interview turns\n",
        "    context: Annotated[list, operator.add]  # Web search results\n",
        "    analyst: MedicalAnalyst  # The analyst\n",
        "    interview: str  # Interview transcript\n",
        "    sections: list  # Final sections\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(description=\"Medical search query for web research\")\n",
        "\n",
        "print(\"Interview state models are defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sp3q8eMMtKh",
        "outputId": "06d2c9c5-e693-416c-da38-03d221bf7e19"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interview state models are defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of Questions\n",
        "\n",
        "The Analysts will generate many relevant questions to ask medical experts:"
      ],
      "metadata": {
        "id": "WYwsD3QRGV2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_instructions = \"\"\"You are a medical analyst interviewing an expert about a health topic.\n",
        "\n",
        "Your goal is to gather specific, evidence-based medical insights.\n",
        "\n",
        "1. Focus on: {goals}\n",
        "\n",
        "2. Ask specific questions about:\n",
        "   - Clinical evidence and research\n",
        "   - Treatment efficacy and safety\n",
        "   - Patient outcomes\n",
        "   - Current medical guidelines\n",
        "\n",
        "3. Avoid vague questions - be specific and clinical\n",
        "\n",
        "Begin by introducing yourself, then ask your medical question.\n",
        "\n",
        "When satisfied, end with: \"Thank you so much for your help!\"\n",
        "\"\"\"\n",
        "\n",
        "def generate_question(state: InterviewState):\n",
        "    \"\"\"Generate analyst question\"\"\"\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    system_message = question_instructions.format(goals=analyst.persona)\n",
        "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    return {\"messages\": [question]}\n",
        "\n",
        "print(\"The Question generation process is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg8SjaEa3HRb",
        "outputId": "370c4ca1-6723-4fc2-9319-dfc7eeebf644"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Question generation process is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Search Functions\n",
        "\n",
        "Now, we will search the web and Wikipedia for getting medical information:"
      ],
      "metadata": {
        "id": "rZGl72hyHOuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "search_instructions = SystemMessage(content=\"\"\"Given a medical conversation, generate a search query to find evidence-based medical information.\n",
        "\n",
        "Focus on:\n",
        "- Peer-reviewed medical sources\n",
        "- Clinical guidelines\n",
        "- Medical research\n",
        "- Reputable health organizations\n",
        "\n",
        "Create a precise medical search query based on the conversation.\"\"\")\n",
        "\n",
        "def search_web(state: InterviewState):\n",
        "    \"\"\"Search web for medical information\"\"\"\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
        "\n",
        "    # Perform web search\n",
        "    try:\n",
        "        search_results = tavily_search.invoke(search_query.search_query)\n",
        "\n",
        "        # Handle different response formats\n",
        "        if isinstance(search_results, list):\n",
        "            search_docs = search_results\n",
        "        elif isinstance(search_results, dict):\n",
        "            search_docs = search_results.get(\"results\", [])\n",
        "        else:\n",
        "            search_docs = []\n",
        "\n",
        "        # Format results\n",
        "        if search_docs:\n",
        "            formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
        "                f'<Document href=\"{doc.get(\"url\", \"N/A\")}\"/>\\n{doc.get(\"content\", doc.get(\"snippet\", \"\"))}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "                if isinstance(doc, dict)\n",
        "            ])\n",
        "        else:\n",
        "            formatted_search_docs = \"No search results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Search error: {e}\")\n",
        "        formatted_search_docs = f\"Search error occurred: {str(e)}\"\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "def search_wikipedia(state: InterviewState):\n",
        "    \"\"\"Search Wikipedia for medical information\"\"\"\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
        "\n",
        "    try:\n",
        "        # Search Wikipedia\n",
        "        search_docs = WikipediaLoader(query=search_query.search_query, load_max_docs=2).load()\n",
        "\n",
        "        # Format results\n",
        "        if search_docs:\n",
        "            formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
        "                f'<Document source=\"{doc.metadata.get(\"source\", \"Wikipedia\")}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "            ])\n",
        "        else:\n",
        "            formatted_search_docs = \"No Wikipedia results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Wikipedia search error: {e}\")\n",
        "        formatted_search_docs = f\"Wikipedia search error: {str(e)}\"\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "print(\" Fixed search functions loaded!\")\n",
        "print(\" Now I will need to rebuild the interview graph - so running the 'Build Interview Graph' cell again!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnoB4OmNHMBL",
        "outputId": "554487bc-0bcf-4ffd-9dee-ea3337e8b12e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fixed search functions loaded!\n",
            " Now I will need to rebuild the interview graph - so running the 'Build Interview Graph' cell again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Medical Expert Answers\n",
        "\n",
        "The medical expert uses web-searched information to answer\\ questions:"
      ],
      "metadata": {
        "id": "HseaaviS3wTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_instructions = \"\"\"You are a medical expert being interviewed.\n",
        "\n",
        "Analyst focus: {goals}\n",
        "\n",
        "Use ONLY the provided medical sources to answer: {context}\n",
        "\n",
        "Guidelines:\n",
        "1. Base answers on evidence from the provided sources\n",
        "2. Cite sources using [1], [2], etc.\n",
        "3. Be specific and clinical\n",
        "4. Note limitations if sources are insufficient\n",
        "5. List sources at the end\n",
        "\n",
        "Provide accurate, evidence-based medical information.\"\"\"\n",
        "\n",
        "def generate_answer(state: InterviewState):\n",
        "    \"\"\"Generate expert answer\"\"\"\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
        "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "    answer.name = \"medical_expert\"\n",
        "\n",
        "    return {\"messages\": [answer]}\n",
        "\n",
        "def save_interview(state: InterviewState):\n",
        "    \"\"\"Save interview transcript\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    interview = get_buffer_string(messages)\n",
        "    return {\"interview\": interview}\n",
        "\n",
        "def route_messages(state: InterviewState, name: str = \"medical_expert\"):\n",
        "    \"\"\"Route between questions and answers\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    max_num_turns = state.get('max_num_turns', 2)\n",
        "\n",
        "    # Count expert responses\n",
        "    num_responses = len([m for m in messages if isinstance(m, AIMessage) and m.name == name])\n",
        "\n",
        "    if num_responses >= max_num_turns:\n",
        "        return 'save_interview'\n",
        "\n",
        "    # Check for end signal\n",
        "    if len(messages) >= 2:\n",
        "        last_question = messages[-2]\n",
        "        if \"Thank you so much for your help\" in last_question.content:\n",
        "            return 'save_interview'\n",
        "\n",
        "    return \"ask_question\"\n",
        "\n",
        "print(\"Answer generation system ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yAcQV0M35Wd",
        "outputId": "d08ac8df-70ea-46f6-8c06-e58613dc9ac6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer generation system ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Interview Graph"
      ],
      "metadata": {
        "id": "PWZ4SOvd4BEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Building the interview graph\n",
        "interview_builder = StateGraph(InterviewState)\n",
        "interview_builder.add_node(\"ask_question\", generate_question)\n",
        "interview_builder.add_node(\"search_web\", search_web)\n",
        "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
        "interview_builder.add_node(\"answer_question\", generate_answer)\n",
        "interview_builder.add_node(\"save_interview\", save_interview)\n",
        "\n",
        "# Defining the flow\n",
        "interview_builder.add_edge(START, \"ask_question\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
        "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
        "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
        "interview_builder.add_conditional_edges(\"answer_question\", route_messages, [\"ask_question\", \"save_interview\"])\n",
        "interview_builder.add_edge(\"save_interview\", END)\n",
        "\n",
        "interview_graph = interview_builder.compile()\n",
        "\n",
        "print(\"Interview graph is compiled!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuoQZ_Jc4FFD",
        "outputId": "05497cc5-d05f-4c86-ebac-834e3c17881e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interview graph is compiled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report Generation\n",
        "\n",
        "Generating the final medical report from all the interviews:"
      ],
      "metadata": {
        "id": "ROSNWwYx5N4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_writer_instructions = \"\"\"You are a medical writer creating a section of a medical report.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Your focus: {focus}\n",
        "\n",
        "Interview transcript: {interview}\n",
        "\n",
        "Create a well-structured section with:\n",
        "1. Clear medical terminology\n",
        "2. Evidence-based information\n",
        "3. Proper citations [1], [2], etc.\n",
        "4. Clinical relevance\n",
        "\n",
        "Write in a professional medical style.\"\"\"\n",
        "\n",
        "def write_section(interview: str, analyst: MedicalAnalyst, topic: str):\n",
        "    \"\"\"Write a section based on interview\"\"\"\n",
        "    system_message = section_writer_instructions.format(\n",
        "        topic=topic,\n",
        "        focus=analyst.description,\n",
        "        interview=interview\n",
        "    )\n",
        "    section = llm.invoke([SystemMessage(content=system_message)])\n",
        "    return section.content\n",
        "\n",
        "report_writer_instructions = \"\"\"You are a medical report compiler creating a comprehensive medical document.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Sections from specialists:\n",
        "{sections}\n",
        "\n",
        "Create a comprehensive medical report with:\n",
        "\n",
        "# {topic}: Complete Medical Overview\n",
        "\n",
        "## Executive Summary\n",
        "[Brief overview]\n",
        "\n",
        "## Detailed Analysis\n",
        "[Integrate all sections logically]\n",
        "\n",
        "## Key Takeaways\n",
        "[Important points]\n",
        "\n",
        "## Sources\n",
        "[All citations]\n",
        "\n",
        "**MEDICAL DISCLAIMER**: This information is for educational purposes only...\n",
        "\"\"\"\n",
        "\n",
        "def compile_report(topic: str, sections: List[str]):\n",
        "    \"\"\"Compile final medical report\"\"\"\n",
        "    formatted_sections = \"\\n\\n\".join(sections)\n",
        "    system_message = report_writer_instructions.format(\n",
        "        topic=topic,\n",
        "        sections=formatted_sections\n",
        "    )\n",
        "    report = llm.invoke([SystemMessage(content=system_message)])\n",
        "    return report.content\n",
        "\n",
        "print(\"The Report generation system is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQED5RXJ5Uiy",
        "outputId": "972dd397-caec-4a6f-acb4-56b9a633d9a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Report generation system is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Medical Research\n",
        "\n",
        "So we will research a medical topic here!\n",
        "We can change the `medical_topic` below:"
      ],
      "metadata": {
        "id": "WmGHB7CrCIhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medical_topic = \"Type 2 Diabetes Mellitus\"  # We can change this to any medical condition\n",
        "num_analysts = 3  # Number of specialist analysts\n",
        "num_interview_turns = 2  # Questions per analyst\n",
        "\n",
        "print(f\" Researching: {medical_topic}\")\n",
        "print(f\" Analysts: {num_analysts}\")\n",
        "print(f\" Interview turns: {num_interview_turns}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3WyKeGyChCn",
        "outputId": "1c730cc4-bf90-4485-e067-cf933bce8626"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Researching: Type 2 Diabetes Mellitus\n",
            " Analysts: 3\n",
            " Interview turns: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Generating the Medical Analysts\n"
      ],
      "metadata": {
        "id": "eL6unm2zCnti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate analysts\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "for event in analyst_graph.stream(\n",
        "    {\"topic\": medical_topic, \"max_analysts\": num_analysts},\n",
        "    thread,\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    analysts = event.get('analysts', '')\n",
        "    if analysts:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" MEDICAL SPECIALIST ANALYSTS GENERATED\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        for analyst in analysts:\n",
        "            print(f\" {analyst.name}\")\n",
        "            print(f\"   Role: {analyst.role}\")\n",
        "            print(f\"   Affiliation: {analyst.affiliation}\")\n",
        "            print(f\"   Focus: {analyst.description}\")\n",
        "            print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htwno3UZC6aZ",
        "outputId": "b303b5a7-741e-4322-a354-93dc40251f6c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " MEDICAL SPECIALIST ANALYSTS GENERATED\n",
            "================================================================================\n",
            "\n",
            " Dr. Sarah Thompson\n",
            "   Role: Symptomatologist\n",
            "   Affiliation: Endocrinology Department\n",
            "   Focus: Dr. Thompson specializes in the identification and analysis of symptoms associated with Type 2 Diabetes Mellitus. Her focus is on understanding the early signs of the disease, such as increased thirst, frequent urination, and unexplained weight loss, to aid in timely diagnosis. She is also concerned with the progression of symptoms and their impact on patients' quality of life.\n",
            "--------------------------------------------------------------------------------\n",
            " Dr. Michael Lee\n",
            "   Role: Treatment Specialist\n",
            "   Affiliation: Diabetes Treatment Center\n",
            "   Focus: Dr. Lee is an expert in the management and treatment of Type 2 Diabetes Mellitus. He focuses on developing personalized treatment plans that include lifestyle modifications, oral medications, and insulin therapy. His expertise extends to the latest advancements in diabetes management technologies and their integration into patient care.\n",
            "--------------------------------------------------------------------------------\n",
            " Dr. Emily Carter\n",
            "   Role: Prevention Expert\n",
            "   Affiliation: Public Health Institute\n",
            "   Focus: Dr. Carter is dedicated to the prevention of Type 2 Diabetes Mellitus through public health initiatives and education. She focuses on identifying risk factors such as obesity, sedentary lifestyle, and genetic predisposition. Her work involves creating community programs aimed at promoting healthy eating, regular physical activity, and early screening to reduce the incidence of the disease.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Conducting Interviews with Web Research"
      ],
      "metadata": {
        "id": "_LCu0DhjDI1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "# Getting the analysts from state\n",
        "final_state = analyst_graph.get_state(thread)\n",
        "analysts = final_state.values.get('analysts')\n",
        "\n",
        "# Conducting the interviews\n",
        "interviews = []\n",
        "\n",
        "for analyst in analysts:\n",
        "    print(f\"\\n Starting the interview with {analyst.name}...\")\n",
        "    print(f\" Focus: {analyst.description}\\n\")\n",
        "\n",
        "    # Running the interview\n",
        "    interview_state = {\n",
        "        \"analyst\": analyst,\n",
        "        \"messages\": [],\n",
        "        \"max_num_turns\": num_interview_turns,\n",
        "        \"context\": []\n",
        "    }\n",
        "\n",
        "    for event in interview_graph.stream(interview_state, stream_mode=\"updates\"):\n",
        "        node_name = next(iter(event.keys()))\n",
        "        print(f\" {node_name}...\")\n",
        "\n",
        "    # Getting the final state\n",
        "    interview_result = interview_graph.invoke(interview_state)\n",
        "    interviews.append(interview_result.get('interview', ''))\n",
        "    print(f\" Interview with {analyst.name} complete!\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL INTERVIEWS ARE COMPLETED\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmDguhHiDKTk",
        "outputId": "6bc553a5-c153-47df-e786-b5382592b100"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Starting the interview with Dr. Sarah Thompson...\n",
            " Focus: Dr. Thompson specializes in the identification and analysis of symptoms associated with Type 2 Diabetes Mellitus. Her focus is on understanding the early signs of the disease, such as increased thirst, frequent urination, and unexplained weight loss, to aid in timely diagnosis. She is also concerned with the progression of symptoms and their impact on patients' quality of life.\n",
            "\n",
            " ask_question...\n",
            " search_wikipedia...\n",
            " search_web...\n",
            " answer_question...\n",
            " save_interview...\n",
            " Interview with Dr. Sarah Thompson complete!\n",
            "\n",
            "\n",
            " Starting the interview with Dr. Michael Lee...\n",
            " Focus: Dr. Lee is an expert in the management and treatment of Type 2 Diabetes Mellitus. He focuses on developing personalized treatment plans that include lifestyle modifications, oral medications, and insulin therapy. His expertise extends to the latest advancements in diabetes management technologies and their integration into patient care.\n",
            "\n",
            " ask_question...\n",
            " search_web...\n",
            " search_wikipedia...\n",
            " answer_question...\n",
            " ask_question...\n",
            " search_web...\n",
            " search_wikipedia...\n",
            " answer_question...\n",
            " save_interview...\n",
            " Interview with Dr. Michael Lee complete!\n",
            "\n",
            "\n",
            " Starting the interview with Dr. Emily Carter...\n",
            " Focus: Dr. Carter is dedicated to the prevention of Type 2 Diabetes Mellitus through public health initiatives and education. She focuses on identifying risk factors such as obesity, sedentary lifestyle, and genetic predisposition. Her work involves creating community programs aimed at promoting healthy eating, regular physical activity, and early screening to reduce the incidence of the disease.\n",
            "\n",
            " ask_question...\n",
            " search_wikipedia...\n",
            " search_web...\n",
            " answer_question...\n",
            " save_interview...\n",
            " Interview with Dr. Emily Carter complete!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ALL INTERVIEWS ARE COMPLETED\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Generating the Medical Report"
      ],
      "metadata": {
        "id": "iEs2HuxVGl2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Writing the report sections...\\n\")\n",
        "\n",
        "# Writing the sections\n",
        "sections = []\n",
        "for i, (analyst, interview) in enumerate(zip(analysts, interviews)):\n",
        "    print(f\" Writing section {i+1}/{len(analysts)}: {analyst.role}...\")\n",
        "    section = write_section(interview, analyst, medical_topic)\n",
        "    sections.append(section)\n",
        "\n",
        "print(\"\\n Compiling the final report...\\n\")\n",
        "\n",
        "# Compiling the report\n",
        "final_report = compile_report(medical_topic, sections)\n",
        "\n",
        "print(\" Report is complete!\\n\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7EQqZLXGn3a",
        "outputId": "a2510689-8992-4d7c-db1b-5075172fe6aa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Writing the report sections...\n",
            "\n",
            " Writing section 1/3: Symptomatologist...\n",
            " Writing section 2/3: Treatment Specialist...\n",
            " Writing section 3/3: Prevention Expert...\n",
            "\n",
            " Compiling the final report...\n",
            "\n",
            " Report is complete!\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weasyprint markdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qZUty4tH-M0",
        "outputId": "cf7af9fe-cce0-430f-e3ba-0ba5746749c2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weasyprint\n",
            "  Downloading weasyprint-66.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (3.10)\n",
            "Collecting pydyf>=0.11.0 (from weasyprint)\n",
            "  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.12/dist-packages (from weasyprint) (2.0.0)\n",
            "Collecting tinyhtml5>=2.0.0b1 (from weasyprint)\n",
            "  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tinycss2>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from weasyprint) (1.4.0)\n",
            "Collecting cssselect2>=0.8.0 (from weasyprint)\n",
            "  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Pyphen>=0.9.1 (from weasyprint)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1.0 in /usr/local/lib/python3.12/dist-packages (from weasyprint) (11.3.0)\n",
            "Requirement already satisfied: fonttools>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from fonttools[woff]>=4.0.0->weasyprint) (4.60.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=0.6->weasyprint) (2.23)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from cssselect2>=0.8.0->weasyprint) (0.5.1)\n",
            "Requirement already satisfied: brotli>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from fonttools[woff]>=4.0.0->weasyprint) (1.2.0)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->weasyprint)\n",
            "  Downloading zopfli-0.4.0-cp310-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading weasyprint-66.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\n",
            "Downloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading zopfli-0.4.0-cp310-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.1/847.1 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zopfli, tinyhtml5, Pyphen, pydyf, cssselect2, weasyprint\n",
            "Successfully installed Pyphen-0.17.2 cssselect2-0.8.0 pydyf-0.11.0 tinyhtml5-2.0.0 weasyprint-66.0 zopfli-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Displaying the Final Medical Report"
      ],
      "metadata": {
        "id": "6CTdKasiHBcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to save as PDF\n",
        "from weasyprint import HTML, CSS\n",
        "import markdown\n",
        "\n",
        "# Converting the markdown to HTML\n",
        "html_content = markdown.markdown(final_report, extensions=['extra', 'nl2br'])\n",
        "\n",
        "# Adding styling\n",
        "styled_html = f\"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <style>\n",
        "        body {{\n",
        "            font-family: Georgia, serif;\n",
        "            line-height: 1.8;\n",
        "            max-width: 800px;\n",
        "            margin: 40px auto;\n",
        "            padding: 20px;\n",
        "            color: #333;\n",
        "        }}\n",
        "        h1 {{\n",
        "            color: #2E86AB;\n",
        "            border-bottom: 3px solid #2E86AB;\n",
        "            padding-bottom: 10px;\n",
        "            font-size: 2.5em;\n",
        "        }}\n",
        "        h2 {{\n",
        "            color: #555;\n",
        "            margin-top: 30px;\n",
        "            font-size: 1.8em;\n",
        "        }}\n",
        "        h3 {{\n",
        "            color: #666;\n",
        "            font-size: 1.4em;\n",
        "        }}\n",
        "        p {{\n",
        "            margin: 15px 0;\n",
        "            text-align: justify;\n",
        "        }}\n",
        "        a {{\n",
        "            color: #2E86AB;\n",
        "            text-decoration: none;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    {html_content}\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Saving this as PDF\n",
        "HTML(string=styled_html).write_pdf(f\"{medical_topic}_report.pdf\")\n",
        "print(f\" PDF is saved as: {medical_topic}_report.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O2iArOYHIq6",
        "outputId": "481d7657-e17b-4428-bc49-5868a23215b4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 41 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'C', 'D', 'E', 'H', 'I', 'K', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'a', 'b', 'c', 'colon', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'two', 'u', 'uni00A0', 'v', 'w', 'x', 'y']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 21, 29, 36, 38, 39, 40, 43, 44, 46, 47, 48, 50, 51, 53, 54, 55, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 41 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'C', 'D', 'E', 'H', 'I', 'K', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'a', 'b', 'c', 'colon', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'two', 'u', 'uni00A0', 'v', 'w', 'x', 'y']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 21, 29, 36, 38, 39, 40, 43, 44, 46, 47, 48, 50, 51, 53, 54, 55, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92]\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 41 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 62 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'C', 'D', 'E', 'G', 'I', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'T', 'a', 'b', 'bracketleft', 'bracketright', 'bullet', 'c', 'colon', 'comma', 'd', 'e', 'f', 'five', 'four', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'period', 'q', 'quotesingle', 'r', 's', 'seven', 'slash', 't', 'three', 'two', 'u', 'underscore', 'uni00A0', 'uni00AD', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 10, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 28, 29, 36, 38, 39, 40, 42, 44, 46, 47, 48, 49, 51, 53, 54, 55, 62, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 533]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 62 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'C', 'D', 'E', 'G', 'I', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'T', 'a', 'b', 'bracketleft', 'bracketright', 'bullet', 'c', 'colon', 'comma', 'd', 'e', 'f', 'five', 'four', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'period', 'q', 'quotesingle', 'r', 's', 'seven', 'slash', 't', 'three', 'two', 'u', 'underscore', 'uni00A0', 'uni00AD', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 10, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 28, 29, 36, 38, 39, 40, 42, 44, 46, 47, 48, 49, 51, 53, 54, 55, 62, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 533]\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 62 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.002s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.002s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1, 31]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PDF is saved as: Type 2 Diabetes Mellitus_report.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some of the Other Examples to Research:\n",
        "\n",
        "Hypertension\n",
        "\n",
        "Arrhythmia\n",
        "\n",
        "Pulmonary embolism\n",
        "\n",
        "Autoimmune disorder\n",
        "\n",
        "Neurodegeneration\n"
      ],
      "metadata": {
        "id": "TuS1TLrVJHd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important Reminder\n",
        "\n",
        "**MEDICAL DISCLAIMER**: This tool is for educational purposes ONLY.\n",
        "The Information provided should NOT replace professional medical advice, diagnosis or treatment.\n",
        "Always consult qualified healthcare providers for medical decisions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wkAuHF68J9Fi"
      }
    }
  ]
}