{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Research Agent\n",
        "\n",
        "# Overview\n",
        "\n",
        "The Medical Research Agent is an AI-powered system designed to process complex medical text and provide clear and structured insights.\n",
        "\n",
        "It can help with understanding:\n",
        "\n",
        "Medical Conditions- diseases, disorders and related issues\n",
        "\n",
        "Medicines- drugs, treatments and their uses\n",
        "\n",
        "Symptoms- what they may indicate and when to be cautious\n",
        "\n",
        "Treatments- available options and important considerations\n",
        "\n",
        "# How It Works\n",
        "\n",
        "Text Understanding- The agent simplifies complex medical text into clear summaries.\n",
        "\n",
        "Information Extraction- It identifies symptoms, causes, and treatment details from the input.\n",
        "\n",
        "Web Retrieval (RAG)- The system fetches verified medical definitions using AI-powered web search.\n",
        "\n",
        "Structured Report Generation- All information is combined into a clean and easy-to-read medical report.\n",
        "\n",
        "# Medical Disclaimer\n",
        "\n",
        "This tool is for educational and informational purposes only.\n",
        "It should not be used as a substitute for professional medical advice, diagnosis, or treatment.\n",
        "Always consult a qualified healthcare provider for medical concerns."
      ],
      "metadata": {
        "id": "CRas6pFem3Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "In this step, we will install all the required packages:"
      ],
      "metadata": {
        "id": "rLKWF6mynSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python langchain-tavily wikipedia"
      ],
      "metadata": {
        "id": "odSDT6lhsHl4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the API Keys\n",
        "\n",
        "So for this project, we will need these 2 API keys:\n",
        "\n",
        "1. **OpenAI API Key**: I got this from https://platform.openai.com/api-keys\n",
        "2. **Tavily API Key**: I got this from https://tavily.com"
      ],
      "metadata": {
        "id": "dVlZ8E78niAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijYqmWuMnUXp",
        "outputId": "07bf35d4-c2d8-4c72-b12f-a4825fa59108"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "TAVILY_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies\n"
      ],
      "metadata": {
        "id": "IxKZ-ELHnp7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, get_buffer_string\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from IPython.display import display, HTML, Image\n",
        "import operator\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Initialize Tavily Search\n",
        "tavily_search = TavilySearch(max_results=3)\n",
        "\n",
        "print(\"All dependencies are loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xzfqweip5DI",
        "outputId": "b87e3937-018b-44f3-bffc-44b4947eb1c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Medical Analyst Models\n",
        "\n",
        "We will create specialized medical analysts:"
      ],
      "metadata": {
        "id": "KfpKHXmewfxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalAnalyst(BaseModel):\n",
        "    \"\"\"Medical specialist analyst\"\"\"\n",
        "    affiliation: str = Field(description=\"Medical affiliation or specialty\")\n",
        "    name: str = Field(description=\"Name of the medical analyst\")\n",
        "    role: str = Field(description=\"Medical role or specialty area\")\n",
        "    description: str = Field(description=\"Focus area, concerns, and medical expertise\")\n",
        "\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "class MedicalPerspectives(BaseModel):\n",
        "    analysts: List[MedicalAnalyst] = Field(\n",
        "        description=\"List of medical analysts with their specialties\"\n",
        "    )\n",
        "\n",
        "class GenerateAnalystsState(TypedDict):\n",
        "    topic: str  # Medical topic or condition\n",
        "    max_analysts: int  # Number of analysts\n",
        "    human_analyst_feedback: str  # Human feedback\n",
        "    analysts: List[MedicalAnalyst]  # Generated analysts\n",
        "\n",
        "print(\"Medical analyst models defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvAjZep5KtEO",
        "outputId": "5faa1bf8-3fdf-41fb-d80a-2ef5cc8e5b07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst models defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Medical Analysts\n",
        "\n",
        "Generating specialized medical analysts for different aspects of the condition:"
      ],
      "metadata": {
        "id": "PkiV8YQ6K02U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyst_instructions = \"\"\"You are tasked with creating a set of medical specialist personas to research a health topic.\n",
        "\n",
        "1. Review the medical topic: {topic}\n",
        "\n",
        "2. Consider any feedback: {human_analyst_feedback}\n",
        "\n",
        "3. Determine the most important medical perspectives (symptoms, treatments, prevention, prognosis, causes, etc.)\n",
        "\n",
        "4. Create {max_analysts} medical specialists, each focusing on a different aspect.\n",
        "\n",
        "Example specialists:\n",
        "- Symptomatologist (focuses on symptoms and diagnosis)\n",
        "- Treatment Specialist (focuses on treatment options)\n",
        "- Prevention Expert (focuses on prevention and risk factors)\n",
        "- Pharmacologist (focuses on medications)\n",
        "\"\"\"\n",
        "\n",
        "def create_medical_analysts(state: GenerateAnalystsState):\n",
        "    \"\"\"Create medical analyst personas\"\"\"\n",
        "    topic = state['topic']\n",
        "    max_analysts = state['max_analysts']\n",
        "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
        "\n",
        "    structured_llm = llm.with_structured_output(MedicalPerspectives)\n",
        "    system_message = analyst_instructions.format(\n",
        "        topic=topic,\n",
        "        human_analyst_feedback=human_analyst_feedback,\n",
        "        max_analysts=max_analysts\n",
        "    )\n",
        "\n",
        "    analysts = structured_llm.invoke([\n",
        "        SystemMessage(content=system_message),\n",
        "        HumanMessage(content=\"Generate the medical specialist analysts.\")\n",
        "    ])\n",
        "\n",
        "    return {\"analysts\": analysts.analysts}\n",
        "\n",
        "def should_continue(state: GenerateAnalystsState):\n",
        "    \"\"\"Check if we should continue or end\"\"\"\n",
        "    if state.get('human_analyst_feedback', None):\n",
        "        return \"create_analysts\"\n",
        "    return END\n",
        "\n",
        "# Build analyst generation graph\n",
        "builder = StateGraph(GenerateAnalystsState)\n",
        "builder.add_node(\"create_analysts\", create_medical_analysts)\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_conditional_edges(\"create_analysts\", should_continue, [\"create_analysts\", END])\n",
        "\n",
        "memory = MemorySaver()\n",
        "analyst_graph = builder.compile(checkpointer=memory)\n",
        "\n",
        "print(\"Medical analyst generation system is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNFqYcwVLNI1",
        "outputId": "bdf7d0e7-4465-4d8e-deb3-8f4da80cd243"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical analyst generation system is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medical Interview System\n",
        "\n",
        "Setting up the interview system where the analysts can collect information from medical experts:"
      ],
      "metadata": {
        "id": "9OBwGQYVMd4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class InterviewState(MessagesState):\n",
        "    max_num_turns: int  # Number of interview turns\n",
        "    context: Annotated[list, operator.add]  # Web search results\n",
        "    analyst: MedicalAnalyst  # The analyst\n",
        "    interview: str  # Interview transcript\n",
        "    sections: list  # Final sections\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(description=\"Medical search query for web research\")\n",
        "\n",
        "print(\"Interview state models are defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sp3q8eMMtKh",
        "outputId": "6f2cde4b-91dc-4d2d-c4e9-7cc65163ee81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interview state models are defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation of Questions\n",
        "\n",
        "The Analysts will generate many relevant questions to ask medical experts:"
      ],
      "metadata": {
        "id": "WYwsD3QRGV2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_instructions = \"\"\"You are a medical analyst interviewing an expert about a health topic.\n",
        "\n",
        "Your goal is to gather specific, evidence-based medical insights.\n",
        "\n",
        "1. Focus on: {goals}\n",
        "\n",
        "2. Ask specific questions about:\n",
        "   - Clinical evidence and research\n",
        "   - Treatment efficacy and safety\n",
        "   - Patient outcomes\n",
        "   - Current medical guidelines\n",
        "\n",
        "3. Avoid vague questions - be specific and clinical\n",
        "\n",
        "Begin by introducing yourself, then ask your medical question.\n",
        "\n",
        "When satisfied, end with: \"Thank you so much for your help!\"\n",
        "\"\"\"\n",
        "\n",
        "def generate_question(state: InterviewState):\n",
        "    \"\"\"Generate analyst question\"\"\"\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    system_message = question_instructions.format(goals=analyst.persona)\n",
        "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    return {\"messages\": [question]}\n",
        "\n",
        "print(\"The Question generation process is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg8SjaEa3HRb",
        "outputId": "57f979a6-a03e-413e-c97f-f0da1431193b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Question generation process is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Search Functions\n",
        "\n",
        "Now, we will search the web and Wikipedia for getting medical information:"
      ],
      "metadata": {
        "id": "rZGl72hyHOuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "search_instructions = SystemMessage(content=\"\"\"Given a medical conversation, generate a search query to find evidence-based medical information.\n",
        "\n",
        "Focus on:\n",
        "- Peer-reviewed medical sources\n",
        "- Clinical guidelines\n",
        "- Medical research\n",
        "- Reputable health organizations\n",
        "\n",
        "Create a precise medical search query based on the conversation.\"\"\")\n",
        "\n",
        "def search_web(state: InterviewState):\n",
        "    \"\"\"Search web for medical information\"\"\"\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
        "\n",
        "    # Perform web search\n",
        "    try:\n",
        "        search_results = tavily_search.invoke(search_query.search_query)\n",
        "\n",
        "        # Handle different response formats\n",
        "        if isinstance(search_results, list):\n",
        "            search_docs = search_results\n",
        "        elif isinstance(search_results, dict):\n",
        "            search_docs = search_results.get(\"results\", [])\n",
        "        else:\n",
        "            search_docs = []\n",
        "\n",
        "        # Format results\n",
        "        if search_docs:\n",
        "            formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
        "                f'<Document href=\"{doc.get(\"url\", \"N/A\")}\"/>\\n{doc.get(\"content\", doc.get(\"snippet\", \"\"))}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "                if isinstance(doc, dict)\n",
        "            ])\n",
        "        else:\n",
        "            formatted_search_docs = \"No search results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Search error: {e}\")\n",
        "        formatted_search_docs = f\"Search error occurred: {str(e)}\"\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "def search_wikipedia(state: InterviewState):\n",
        "    \"\"\"Search Wikipedia for medical information\"\"\"\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
        "\n",
        "    try:\n",
        "        # Search Wikipedia\n",
        "        search_docs = WikipediaLoader(query=search_query.search_query, load_max_docs=2).load()\n",
        "\n",
        "        # Format results\n",
        "        if search_docs:\n",
        "            formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
        "                f'<Document source=\"{doc.metadata.get(\"source\", \"Wikipedia\")}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "            ])\n",
        "        else:\n",
        "            formatted_search_docs = \"No Wikipedia results found.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Wikipedia search error: {e}\")\n",
        "        formatted_search_docs = f\"Wikipedia search error: {str(e)}\"\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "print(\" Fixed search functions loaded!\")\n",
        "print(\" Now I will need to rebuild the interview graph - run the 'Build Interview Graph' cell again!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnoB4OmNHMBL",
        "outputId": "21564e15-807f-4cef-d86e-922fce1cb07d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fixed search functions loaded!\n",
            " Now I will need to rebuild the interview graph - run the 'Build Interview Graph' cell again!\n"
          ]
        }
      ]
    }
  ]
}